{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mDynamic and fast\n",
      "JavaScript weaves magic code\n",
      "Web's heart beats with it\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThis Haiku is describing JavaScript, a popular programming language used for building dynamic and interactive websites. The first line, \"Dynamic and fast,\" highlights the dynamic nature of JavaScript, which allows for real-time changes and updates on web pages. The second line, \"JavaScript weaves magic code,\" suggests that JavaScript has the ability to create powerful and engaging features on websites. Finally, the third line, \"Web's heart beats with it,\" emphasizes the importance of JavaScript in the functioning and overall user experience of the web. Overall, this Haiku captures the essence of JavaScript as a versatile and essential tool for web development.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This Haiku is describing JavaScript, a popular programming language used for building dynamic and interactive websites. The first line, \"Dynamic and fast,\" highlights the dynamic nature of JavaScript, which allows for real-time changes and updates on web pages. The second line, \"JavaScript weaves magic code,\" suggests that JavaScript has the ability to create powerful and engaging features on websites. Finally, the third line, \"Web\\'s heart beats with it,\" emphasizes the importance of JavaScript in the functioning and overall user experience of the web. Overall, this Haiku captures the essence of JavaScript as a versatile and essential tool for web development.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate,PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# First Chain: Generates a Haiku about a given programming language\n",
    "haiku_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a Haiku about the programming language {language}. It should capture its essence and characteristics.\"\n",
    ")\n",
    "haiku_chain = LLMChain(llm=ChatOpenAI(model=\"gpt-3.5-turbo\"), prompt=haiku_prompt)\n",
    "\n",
    "# Second Chain: Explains the Haiku\n",
    "explanation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain the meaning of the following Haiku about a programming language:\\n\\n{haiku}\\n\\nProvide an insightful explanation.\"\n",
    ")\n",
    "explanation_chain = LLMChain(llm=ChatOpenAI(model=\"gpt-3.5-turbo\"), prompt=explanation_prompt)\n",
    "\n",
    "# Chain them together\n",
    "final_chain = SimpleSequentialChain(chains=[haiku_chain, explanation_chain], verbose=True)\n",
    "\n",
    "final_chain.run(\"javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ì œëª©:** ê°€ë‚˜ì˜ í˜¼ì¸ì”ì¹˜\n",
      "**ê°ë…:** ì¡°ì§€ í´ë£¨ë‹ˆ\n",
      "**ì£¼ì—°:** ì¡°ì§€ í´ë£¨ë‹ˆ, ì• ë¯¸ ì•„ë‹´ìŠ¤, ì¡´ íŠ¸ë¼ë³¼íƒ€, í¬ë¦¬ìŠ¤í† í¼ ì›Œì»¨\n",
      "**ì˜ˆì‚°:** ì•Œë ¤ì§€ì§€ ì•ŠìŒ\n",
      "**ë°•ìŠ¤ì˜¤í”¼ìŠ¤ ìˆ˜ìµ:** 3,100ë§Œ ë‹¬ëŸ¬\n",
      "**ì¥ë¥´:** ì½”ë¯¸ë””, ë“œë¼ë§ˆ\n",
      "**ì‹œë†‰ì‹œìŠ¤:** 1950ë…„ëŒ€ ë¯¸êµ­, ë¹ˆê³¤í•œ ê°€ë‚˜ì™€ ê·¸ì˜ ì•„ë‚´ ë¡œì¦ˆëŠ” ë¶€ì ê°€ì¡±ì˜ í˜¼ì¸ì‹ì— ì´ˆëŒ€ë°›ê²Œ ëœë‹¤. ê·¸ëŸ¬ë‚˜ ê°€ë‚œí•œ ê°€ë‚˜ì™€ ë¡œì¦ˆê°€ í˜¼ì¸ì‹ì— ì°¸ì„í•˜ê²Œ ë˜ë©´ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì‚¬ê±´ì´ ë²Œì–´ì§€ê²Œ ëœë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "# âœ… ì˜ˆì œ ë°ì´í„° (Few-Shot Learning)\n",
    "examples = [\n",
    "    {\n",
    "        \"movie\": \"ì¸ì…‰ì…˜\",\n",
    "        \"response\": (\n",
    "            \"**ì œëª©:** ì¸ì…‰ì…˜\\n\"\n",
    "            \"**ê°ë…:** í¬ë¦¬ìŠ¤í† í¼ ë†€ë€\\n\"\n",
    "            \"**ì£¼ì—°:** ë ˆì˜¤ë‚˜ë¥´ë„ ë””ì¹´í”„ë¦¬ì˜¤, ì¡°ì…‰ ê³ ë“  ë ˆë¹—, ì—˜ë¦¬ì—‡ í˜ì´ì§€, í†° í•˜ë””\\n\"\n",
    "            \"**ì˜ˆì‚°:** 1ì–µ 6ì²œë§Œ ë‹¬ëŸ¬\\n\"\n",
    "            \"**ë°•ìŠ¤ì˜¤í”¼ìŠ¤ ìˆ˜ìµ:** 8ì–µ 2,990ë§Œ ë‹¬ëŸ¬\\n\"\n",
    "            \"**ì¥ë¥´:** SF, ìŠ¤ë¦´ëŸ¬\\n\"\n",
    "            \"**ì‹œë†‰ì‹œìŠ¤:** ê¿ˆì†ì—ì„œ ì •ë³´ë¥¼ í›”ì¹˜ëŠ” íŠ¹ìˆ˜ ìš”ì› 'ë” ì½”ë¸Œ'ëŠ” ê¸°ì–µì„ ì§€ìš¸ ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ì„ë¬´ë¥¼ ë§¡ê²Œ ëœë‹¤. ê·¸ëŸ¬ë‚˜ ê¿ˆì˜ ì„¸ê³„ì—ì„œ í˜„ì‹¤ê³¼ í™˜ìƒì´ ë’¤ì„ì´ë©° ì˜ˆìƒì¹˜ ëª»í•œ ìœ„ê¸°ê°€ ë‹¥ì¹œë‹¤.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"movie\": \"ëŒ€ë¶€\",\n",
    "        \"response\": (\n",
    "            \"**ì œëª©:** ëŒ€ë¶€\\n\"\n",
    "            \"**ê°ë…:** í”„ë€ì‹œìŠ¤ í¬ë“œ ì½”í´ë¼\\n\"\n",
    "            \"**ì£¼ì—°:** ë§ë¡  ë¸Œë€ë„, ì•Œ íŒŒì¹˜ë…¸, ì œì„ìŠ¤ ì¹¸, ë‹¤ì´ì•ˆ í‚¤íŠ¼\\n\"\n",
    "            \"**ì˜ˆì‚°:** 600~720ë§Œ ë‹¬ëŸ¬\\n\"\n",
    "            \"**ë°•ìŠ¤ì˜¤í”¼ìŠ¤ ìˆ˜ìµ:** 2ì–µ 5ì²œë§Œ~2ì–µ 8ì²œ 7ë°±ë§Œ ë‹¬ëŸ¬\\n\"\n",
    "            \"**ì¥ë¥´:** ë²”ì£„, ë“œë¼ë§ˆ\\n\"\n",
    "            \"**ì‹œë†‰ì‹œìŠ¤:** ë‰´ìš• ë§ˆí”¼ì•„ì˜ ëŒ€ë¶€ ë¹„í†  ì½”ë¥¼ë ˆì˜¤ë„¤ëŠ” ì¡°ì§ì„ ì´ëŒì–´ê°€ì§€ë§Œ, ì•”ì‚´ ì‹œë„ ì´í›„ ì•„ë“¤ ë§ˆì´í´ ì½”ë¥¼ë ˆì˜¤ë„¤ê°€ ê°€ì—…ì„ ë¬¼ë ¤ë°›ìœ¼ë©° ë³€í™”ê°€ ì°¾ì•„ì˜¨ë‹¤.\"\n",
    "        ),\n",
    "    }\n",
    "]\n",
    "\n",
    "# âœ… ì˜ˆì œ ì¶œë ¥ í˜•ì‹ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"ì˜í™” ì œëª©: {movie}\\nì‘ë‹µ:\\n{response}\"\n",
    ")\n",
    "\n",
    "# âœ… FewShotPromptTemplate ìƒì„±\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"ë‹¹ì‹ ì€ ì˜í™” ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì˜í™” ì œëª©ì— ëŒ€í•´ í•­ìƒ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\",\n",
    "    suffix=\"ì˜í™” ì œëª©: {movie}\\nì‘ë‹µ:\",\n",
    "    input_variables=[\"movie\"]\n",
    ")\n",
    "\n",
    "# âœ… LLM ì²´ì¸ ìƒì„±\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "movie_chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "\n",
    "# âœ… ì˜í™” ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_movie_details(movie_name):\n",
    "    return movie_chain.run({\"movie\": movie_name})\n",
    "\n",
    "# âœ… ì‹¤í–‰ ì˜ˆì œ\n",
    "movie_name = \"ê°€ë‚˜ì˜ í˜¼ì¸ì”ì¹˜\"  # ì›í•˜ëŠ” ì˜í™” ì œëª©ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥\n",
    "result = get_movie_details(movie_name)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# âœ… Few-Shot ì˜ˆì œ ë°ì´í„° (í•­ìƒ 3ê°œì˜ ì´ëª¨ì§€ë¥¼ ë°˜í™˜í•˜ë„ë¡ ìœ ë„)\n",
    "examples = [\n",
    "    {\"movie\": \"Top Gun\", \"emojis\": \"ğŸ›©ï¸ğŸ‘¨â€âœˆï¸ğŸ”¥\"},\n",
    "    {\"movie\": \"The Godfather\", \"emojis\": \"ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ğŸ”«ğŸ\"},\n",
    "    {\"movie\": \"Finding Nemo\", \"emojis\": \"ğŸ ğŸŒŠğŸ¦ˆ\"},\n",
    "    {\"movie\": \"Titanic\", \"emojis\": \"ğŸš¢â¤ï¸ğŸ’”\"},\n",
    "    {\"movie\": \"Harry Potter\", \"emojis\": \"âš¡ğŸ“–ğŸ§™\"},\n",
    "]\n",
    "\n",
    "# âœ… FewShotPromptTemplate ì„¤ì •\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"movie\", \"emojis\"],\n",
    "    template=\"Movie: {movie}\\n Emojis: {emojis}\"\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"You are a movie expert. Based on the movie title, provide exactly 3 emojis that best represent it.\\n\\nPrevious History:\\n{chat_history}\\n\\n\",\n",
    "    suffix=\"Movie: {movie}\\nEmojis:\",\n",
    "    input_variables=[\"movie\"]\n",
    ")\n",
    "\n",
    "# âœ… Memory ì„¤ì • (ë©”ëª¨ë¦¬ ìœ ì§€)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",max_token_limit=100 , return_messages=True,k=5)\n",
    "\n",
    "# âœ… LLM ì²´ì¸ ìƒì„±\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "emoji_chain = LLMChain(llm=llm, prompt=few_shot_prompt, memory=memory)\n",
    "\n",
    "# âœ… ì²´ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def get_movie_emojis(movie_name):\n",
    "    return emoji_chain.run({\"movie\": movie_name})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First movie (Avatar): ğŸŒğŸ’™ğŸ‘½\n",
      "Second movie (The Matrix): ğŸ•¶ï¸ğŸ’»ğŸ”µ\n",
      "Third movie (Jurassic Park): ğŸ¦•ğŸŒ´ğŸ”¬\n",
      "Fourth movie (Star Wars): ğŸŒŒâš”ï¸ğŸ¤–\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì²« ë²ˆì§¸ ì˜í™” ì§ˆë¬¸\n",
    "first_movie = \"Avatar\"\n",
    "first_result = get_movie_emojis(first_movie)\n",
    "print(f\"First movie ({first_movie}): {first_result}\")\n",
    "\n",
    "# âœ… ë‘ ë²ˆì§¸ ì˜í™” ì§ˆë¬¸\n",
    "second_movie = \"The Matrix\"\n",
    "second_result = get_movie_emojis(second_movie)\n",
    "print(f\"Second movie ({second_movie}): {second_result}\")\n",
    "\n",
    "# âœ… ì„¸ ë²ˆì§¸ ì˜í™” ì§ˆë¬¸\n",
    "third_movie = \"Jurassic Park\"\n",
    "third_result = get_movie_emojis(third_movie)\n",
    "print(f\"Third movie ({third_movie}): {third_result}\")\n",
    "\n",
    "# âœ… ë„¤ ë²ˆì§¸ ì˜í™” ì§ˆë¬¸ (ì´ì œ ì²« ë²ˆì§¸ ì˜í™”ëŠ” ë©”ëª¨ë¦¬ì—ì„œ ì‚¬ë¼ì§)\n",
    "fourth_movie = \"Star Wars\"\n",
    "fourth_result = get_movie_emojis(fourth_movie)\n",
    "print(f\"Fourth movie ({fourth_movie}): {fourth_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result ğŸŒğŸ’™ğŸ‘½\n",
      "Memory Recall: {'chat_history': [HumanMessage(content='Avatar'), AIMessage(content='ğŸŒğŸ’™ğŸ‘½'), HumanMessage(content='The Matrix'), AIMessage(content='ğŸ•¶ï¸ğŸ’»ğŸ”µ'), HumanMessage(content='Jurassic Park'), AIMessage(content='ğŸ¦•ğŸŒ´ğŸ”¬'), HumanMessage(content='Star Wars'), AIMessage(content='ğŸŒŒâš”ï¸ğŸ¤–'), HumanMessage(content='Avatar'), AIMessage(content='ğŸŒğŸ’™ğŸ‘½')]}\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì´ì „ì— ì§ˆë¬¸í•œ ì˜í™” ê¸°ì–µí•˜ëŠ”ì§€ í™•ì¸\n",
    "previous_result = get_movie_emojis(\"Avatar\")\n",
    "print(\"result\", previous_result)\n",
    "previous_question = emoji_chain.memory.load_memory_variables({})\n",
    "print(\"Memory Recall:\", previous_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1444, which is longer than the specified 1000\n",
      "Created a chunk of size 1251, which is longer than the specified 1000\n",
      "Created a chunk of size 1012, which is longer than the specified 1000\n",
      "Created a chunk of size 2313, which is longer than the specified 1000\n",
      "Created a chunk of size 1458, which is longer than the specified 1000\n",
      "Created a chunk of size 1673, which is longer than the specified 1000\n",
      "Created a chunk of size 1137, which is longer than the specified 1000\n",
      "Created a chunk of size 1559, which is longer than the specified 1000\n",
      "Created a chunk of size 1200, which is longer than the specified 1000\n",
      "Created a chunk of size 1042, which is longer than the specified 1000\n",
      "Created a chunk of size 1345, which is longer than the specified 1000\n",
      "Created a chunk of size 1339, which is longer than the specified 1000\n",
      "Created a chunk of size 1288, which is longer than the specified 1000\n",
      "Created a chunk of size 1014, which is longer than the specified 1000\n",
      "Created a chunk of size 1178, which is longer than the specified 1000\n",
      "Created a chunk of size 2247, which is longer than the specified 1000\n",
      "Created a chunk of size 1728, which is longer than the specified 1000\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['question', 'summaries'] (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# âœ… 7. StuffDocuments Chain ìˆ˜ë™ êµ¬ì„±\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval_qa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[0;32m---> 39\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstuff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_source_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/fullstack-gpt/env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:100\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 100\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_chain_type_kwargs\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/project/fullstack-gpt/env/lib/python3.10/site-packages/langchain/chains/question_answering/__init__.py:249\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/fullstack-gpt/env/lib/python3.10/site-packages/langchain/chains/question_answering/__init__.py:81\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m     74\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     75\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStuffDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/fullstack-gpt/env/lib/python3.10/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/project/fullstack-gpt/env/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['question', 'summaries'] (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "import requests\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.qa_with_sources.stuff_prompt import PROMPT\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# âœ… 1. ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (from Gist)\n",
    "url = \"https://gist.githubusercontent.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223/raw\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# âœ… 2. ë¬¸ì„œë¥¼ LangChain ë¬¸ì„œ í˜•íƒœë¡œ ë³€í™˜\n",
    "docs = [Document(page_content=text)]\n",
    "\n",
    "# âœ… 3. ë¬¸ì„œ ë¶„í•  (StuffDocumentsëŠ” ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ RAG íš¨ìœ¨ì„ ìœ„í•´ ë‚˜ëˆ•ë‹ˆë‹¤)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# âœ… 4. ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• (Embedding + Vectorstore)\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding)\n",
    "\n",
    "# âœ… 5. Retriever ì„¤ì •\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# âœ… 6. Memory ì„¤ì •\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# âœ… 7. StuffDocuments Chain ìˆ˜ë™ êµ¬ì„±\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\"),\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
